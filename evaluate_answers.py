from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from typing import List
import pandas as pd
import numpy as np
 
from data_loader import load_qa
from rag_pipeline import rag_pipeline
from prompts import improved_judge_prompt


class Eval_LLM_QA:
    def __init__(self, llms_answers: List[str], true_answers: List[str], questions: List[str], generator_model_name: str):
        assert len(llms_answers) == len(true_answers), "Both lists must have the same length"
        assert len(llms_answers) == len(questions), "Both lists must have the same length"
        self.llms_answers = llms_answers
        self.true_answers = true_answers
        self.questions = questions
        self.output_folder = "./output/"
        self.saved_path = self.output_folder + "evaluation_scores_" + generator_model_name + ".csv"
        self.batch_size = 4
        

    def traditional_evaluate_answers(self) -> dict:
        """
        Evaluates the similarity between LLM-generated answers and true answers using BLEU, ROUGE, and cosine similarity.
        """
    
        rouge = Rouge()
        tfidf_vectorizer = TfidfVectorizer()
        
        bleu_scores = []
        rouge_scores = []
        cosine_scores = []
        
        for llm_ans, true_ans in zip(self.llms_answers, self.true_answers):
            # BLEU Score
            bleu_score = sentence_bleu([true_ans.split()], llm_ans.split())
            bleu_scores.append(bleu_score)
            
            # ROUGE Score
            rouge_score = rouge.get_scores(llm_ans, true_ans)[0]['rouge-l']['f']
            rouge_scores.append(rouge_score)
            
            # Cosine Similarity
            vectors = tfidf_vectorizer.fit_transform([llm_ans, true_ans])
            cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]
            cosine_scores.append(cosine_sim)
        
        pd.DataFrame({"blue": bleu_scores, "rouge": rouge_scores, "cosine": cosine_scores}).to_csv(self.saved_path , index=False)
        return {
            "BLEU Scores": np.mean(bleu_scores),
            "ROUGE Scores": np.mean(rouge_scores),
            "Cosine Similarity": np.mean(cosine_scores)
        }
    

    def evaluate_judge_llm(self, judge_model_name: str) -> dict:
        result = {"judge_scores": []}
        llm_judge = rag_pipeline(judge_model_name).load_pipeline()
        prompt = improved_judge_prompt(judge_model_name)
        self.prompt_chain = prompt | llm_judge

        llm_input = [{"question": self.questions[i], "answer": self.llms_answers[i]} for i in range(len(self.llms_answers))]

        # Run the pipeline and get the scores from the judge LLM
        print("Answers to judge: ", len(llm_input))
        for i in range(0, len(llm_input), self.batch_size):
            print(i)
            res = self.prompt_chain.batch(llm_input[i:i+self.batch_size])
            for j in range(len(res)):
                try: 
                    result["judge_scores"].append(int(res[j][-1]))
                except ValueError or TypeError:         
                    result["judge_scores"].append(res[j])         
                    print(res[j])
                    print("----------------")

        df = pd.read_csv(self.saved_path)
        df["judge_scores"] = result["judge_scores"]
        df.to_csv(self.saved_path, index=False)

        return result
    
if __name__ == "__main__":
    
    dataset = load_qa()
    llm_answers = pd.read_csv("output/output_Llama-2-7b-chat-hf.csv")["answer"]
    eval = Eval_LLM_QA(llm_answers, dataset["answer"], dataset["question"], "Llama-2-7b-chat-hf")

    # The following code will do an unbiased evaluation and is only based on 
    # comparing the ground truth answers with the LLM generated answers.
    result = eval.traditional_evaluate_answers()
    print(result)

    # The following evaluation is based on a score ruberic given to a judge LLM to 
    # rate the answers generated by another LLM. 
    # Here the aim is to see if the judge LLM would prefer the answers from it's own family of LLM models. 
    result = eval.evaluate_judge_llm("Meta-Llama-3-8B-Instruct")
    print(result)