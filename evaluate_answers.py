from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from typing import List
import pandas as pd
import numpy as np
 
from data_loader import load_qa
from rag_pipeline import rag_pipeline
from prompts import improved_judge_prompt


class Eval_LLM_QA:
    def __init__(self, llms_answers: List[str], true_answers: List[str], questions: List[str]):
        assert len(llms_answers) == len(true_answers), "Both lists must have the same length"
        assert len(llms_answers) == len(questions), "Both lists must have the same length"
        self.llms_answers = llms_answers[:5]
        self.true_answers = true_answers[:5]
        self.questions = questions[:5]
        

    def traditional_evaluate_answers(self) -> dict:
        """
        Evaluates the similarity between LLM-generated answers and true answers using BLEU, ROUGE, and cosine similarity.
        """
    
        rouge = Rouge()
        tfidf_vectorizer = TfidfVectorizer()
        
        bleu_scores = []
        rouge_scores = []
        cosine_scores = []
        
        for llm_ans, true_ans in zip(self.llms_answers, self.true_answers):
            # BLEU Score
            bleu_score = sentence_bleu([true_ans.split()], llm_ans.split())
            bleu_scores.append(bleu_score)
            
            # ROUGE Score
            rouge_score = rouge.get_scores(llm_ans, true_ans)[0]['rouge-l']['f']
            rouge_scores.append(rouge_score)
            
            # Cosine Similarity
            vectors = tfidf_vectorizer.fit_transform([llm_ans, true_ans])
            cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]
            cosine_scores.append(cosine_sim)
        
        return {
            "BLEU Scores": np.mean(bleu_scores),
            "ROUGE Scores": np.mean(rouge_scores),
            "Cosine Similarity": np.mean(cosine_scores)
        }
    

    def evaluate_judge_llm(self, judge_model_name: str) -> dict:

        rag_pipeline_judge = rag_pipeline(judge_model_name).load_pipeline()
        prompt = improved_judge_prompt(judge_model_name)
        self.prompt_chain = prompt | rag_pipeline_judge

        # TODO run the pipeline and get the scores from the judge LLM
        pass

        

if __name__ == "__main__":
    dataset = load_qa()
    llm_answers = pd.read_csv("output/output_Llama-2-7b-chat-hf.csv")["answer"]
    eval = Eval_LLM_QA(llm_answers, dataset["answer"], dataset["question"])

    # The following code will do an unbiased evaluation and is only based on 
    # comparing the ground truth answers with the LLM generated answers.
    result = eval.traditional_evaluate_answers()
    print(result)

    # The following evaluation is based on a score ruberic given to a judge LLM to 
    # rate the answers generated by another LLM. 
    # Here the aim is to see if the judge LLM would prefer the answers from it's own family of LLM models. 
    result = eval.evaluate_judge_llm("llama3")
    print(result)